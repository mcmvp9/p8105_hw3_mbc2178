---
title: "p8105_hw3_mbc2178"
author: "Melvin Coleman"
date: "2022-10-11"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  fig.width = 6,
  out.width = "90%",
  fig.height = 6)
  
```

Let's load in the packages needed to perform data import,manipulation and cleaning for this assignment.

```{r message = FALSE}
library(tidyverse)
library(dplyr)
library(readr)
library(p8105.datasets)
library(ggridges)
library(patchwork)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1 

Let's load the `instacart` data from the `p8105.datasets` using the following:
```{r}
data("instacart") 

instacart = 
  instacart %>% 
  as_tibble(instacart)
```

The `instacart` dataset contains `r nrow(instacart)` observation 
and `r ncol(instacart)` variables. The variables that exist in this dataset include 
`r ls(instacart)`. 


## Problem 2 

Let's import the `accel_data` dataset to R. This dataset contains five weeks of 
accelorometer data collected on a 63 year old male with BMI 25, who was admitted 
to the Advanced Cardiac Care Center of Columbia University Medical Center and diagnosed 
with congestive hear failure (CHF).

We will perform some tidying and wrangle the data before proceeding to perform 
any analysis. First, we will consider using `janitor::clean_names()` to clean 
the names of the variables in the dataset. Next, we will use the `pivot_longer`
function to create a new variable that contains the activity count names and 
another variable that contains the activity counts for each munute of a 24-hr 
day starting at midnight. We arranged dataset according to day of week and 
converted day to a factor variable for 

```{r , results=hide}
accel_df =
  read_csv(file = "data/accel_data.csv") %>% 
  janitor::clean_names() %>% 
  pivot_longer(
    activity_1:activity_1440,
    names_to = "activity",
    names_prefix= "activity_",
    values_to = "activity_cnt"
  ) %>% 
  mutate(
    day = factor(day,levels =(c("Monday","Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"))),
    activity = as.numeric(activity),
    
    day_of_week = case_when(
    day == "Monday" ~ "Weekday",
    day == "Tuesday" ~ "Weekday",
    day == "Wednesday" ~ "Weekday",
    day == "Thursday" ~ "Weekday",
    day =="Friday" ~ "Weekend",
    day == "Saturday" ~ "Weekend",
    day == "Sunday" ~ "Weekend",
  )) %>% 
  arrange(day_of_week)
 
```

The `accel_df` consists of `r nrow(accel_df)` observations and `r ncol(accel_df)` variables. 
The variables in this dataset include `r(accel_df)`
The maximum amount of activites that the patient engaged in was `r max(accel_df$activity_cnt)`
Maybe can add mean amount of mins 

Now let's use our tidied dataset to understand the total activity over the day. We will aggregate 
across minutes to create a total activity variable for each and create a table showing these totals. 
We generated a table below in descending order of total minutes for each day.

```{r}
accel_df %>% 
  group_by(day, week) %>% 
  summarize(total_acitvity_cnt = sum(activity_cnt)) %>% 
  pivot_wider(
    names_from = day,
    values_from = total_acitvity_cnt
  ) %>% 
  knitr::kable(digits = 1)
```


Now, let's create a single panel plot that shows the 24 hour activity courses for each day 
and use color to indicate the day of the week. 

```{r}
accel_df %>% 
  ggplot(aes(x=activity, y =activity_cnt, color = day)) +
  geom_line(alpha = .3) 
  
```

Provide description of what's going on here NEED TO FIGURE OUT SORTING ORDER on graph!!!

##Problem 3 


Let's load the `ny_noaa` dataset to answer problem 3 using the code below.

```{r}
data("ny_noaa") 
```
Wtite short description describing dataset, size and structure, variables missing data

about one half of the stations report precipitation only. Both the record length and period of record vary by station and cover intervals ranging from less than a year to more than 175 years.


Now we perform some data cleaning 
ensure that observations for temperature, precipitation, and snowfall are given in reasonable units
Let's perform some data cleaning on this dataset. We first separate the variable
`date` into three separate variables, `year`, `month`, and `day`. Convert 

precipitation converted to mm (0.1mm = 0.01cm) divide by 10
snowfall converted to cm(1mm = 0.1cm) divide by 10
temperature converted to C (0.1 C = 1C) divide by 10
tmax same conversions too

```{r}
noaa_df = 
  ny_noaa %>% 
  as_tibble(ny_noaa)%>% 
  janitor::clean_names() %>% 
  separate(date, into = c("year", "month", "day")) %>% 
  mutate(
      month= month.name[as.numeric(month)],
      year = as.numeric(year),
      tmax = as.numeric(tmax),
      tmin = as.numeric(tmin)) %>% 
  mutate(
    tmax = tmax / 10,
    tmin = tmin / 10,
    prcp = prcp / 10,
    snow = snow / 10
  ) 
```

Let's find the count of most common values for snow.

```{r}
noaa_df %>% 
  count(snow, name = "n_obs")
```

The most commonly observed value for snowfall observed from this dateset is 0.0cm. 
This is probably becuase it doesn't snow a lot in NY. There are many days where 
NY doesn;t receive a ton of snow. 
 New York state weather stations from January 1, 1981 through December 31, 2010.



Now let's make a two panel plot showing the average max temperature in January and in
July in each station across years. 

```{r}
noaa_df %>% 
  group_by(month,year) %>%
  filter(month == c("01","07")) %>% 
  summarize(
   mean_tmax = mean(tmax, na.rm = TRUE)) %>% 
  ggplot(aes(x =year, y = mean_tmax, color = month)) +
  geom_boxplot(alpha = .5) +
   facet_grid(. ~month)
 
  
```
Now, we make a two-panel plot showing tmax vs tmin for the full dataset. 
```{r}


```

Let's make aother plot showing the distribtuion of snowfall values greater than 0 and less 
than 100 separately by year.

```{r}
noaa_df %>% 
  group_by(year, snow) %>% 
  filter(snow < 100) %>% 
  summarize()
```









