---
title: "p8105_hw3_mbc2178"
author: "Melvin Coleman"
date: "2022-10-11"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  fig.width = 6,
  out.width = "90%",
  fig.height = 6)
  
```

Let's load in the packages needed to perform data import,manipulation and cleaning for this assignment.

```{r message = FALSE}
library(tidyverse)
library(dplyr)
library(readr)
library(p8105.datasets)
library(ggridges)
library(patchwork)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1 

Let's load the `instacart` data from the `p8105.datasets` using the following:
```{r}
data("instacart") 

instacart = 
  instacart %>% 
  as_tibble(instacart)
```

The `instacart` dataset contains `r nrow(instacart)` observation 
and `r ncol(instacart)` variables. The variables that exist in this dataset include 
identifiers, orders, aisle and name of products. There are 
`r instacart %>% select(product_id) %>% distinct %>% count` products found in 
`r instacart %>% select(user_id, order_id) %>% distinct %>% count` orders from 
`r instacart %>% select(user_id) %>% distinct %>% count` buyers.

There are 134 aisles and fresh vegetables and fruits are by far the most ordered items.
The table below provides this count. 

```{r}
instacart %>% 
  count(aisle) %>% 
  arrange(desc(n))
```
Now, let's make a plot that shows the number of items ordered in each aisle, limiting 
this to aisles with more than 10000 items ordered arranged in ascending order by aisles. 

```{r}
instacart %>% 
  count(aisle) %>% 
  filter(n > 10000) %>% 
  mutate(aisle = fct_reorder(aisle, n)) %>% 
  ggplot(aes(x = aisle, y = n)) + 
  geom_point() + 
  labs(title = "Number of items ordered in each aisle") +
  theme(axis.text.x = element_text(angle = 60, hjust = 1))

```
The table below shows the three most popular items in each aisles 
"baking ingredients", "dog food care", and "packaged vegetables fruits". We show 
the name and count of the most popular items per category and rank them in the table
below.

```{r}
instacart %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>%
  group_by(aisle) %>% 
  count(product_name) %>% 
  mutate(rank = min_rank(desc(n))) %>% 
  filter(rank < 4) %>% 
  arrange(desc(n)) %>%
  knitr::kable()
```

Last, we make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are 
ordered on each day of the week. Coffee Ice Cream seem to be ordered more frequently 
earlier on in the day.

```{r}
instacart %>%
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>%
  group_by(product_name, order_dow) %>%
  summarize(mean_hour = mean(order_hour_of_day)) %>%
  spread(key = order_dow, value = mean_hour) %>%
  knitr::kable(digits = 2)
```


## Problem 2 

Let's import the `accel_data` dataset to R. This dataset contains five weeks of 
accelorometer data collected on a 63 year old male with BMI 25, who was admitted 
to the Advanced Cardiac Care Center of Columbia University Medical Center and diagnosed 
with congestive hear failure (CHF).

We will perform some tidying and wrangle the data before proceeding to perform 
any analysis. First, we will consider using `janitor::clean_names()` to clean 
the names of the variables in the dataset. Next, we will use the `pivot_longer`
function to create a new variable that contains the activity count names and 
another variable that contains the activity counts for each munute of a 24-hr 
day starting at midnight. We arranged dataset according to day of week and 
converted day to a factor variable for 

```{r , results=hide}
accel_df =
  read_csv(file = "data/accel_data.csv") %>% 
  janitor::clean_names() %>% 
  pivot_longer(
    activity_1:activity_1440,
    names_to = "activity",
    names_prefix= "activity_",
    values_to = "activity_cnt"
  ) %>% 
  mutate(
    day = factor(day,levels =(c("Monday","Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"))),
    activity = as.numeric(activity),
    
    day_of_week = case_when(
    day == "Monday" ~ "Weekday",
    day == "Tuesday" ~ "Weekday",
    day == "Wednesday" ~ "Weekday",
    day == "Thursday" ~ "Weekday",
    day =="Friday" ~ "Weekend",
    day == "Saturday" ~ "Weekend",
    day == "Sunday" ~ "Weekend",
  )) %>% 
  arrange(day_of_week)
 
```

The `accel_df` consists of `r nrow(accel_df)` observations and `r ncol(accel_df)` variables. 
The variables in this data set include day id, week, number of activity and activity counts 
measured in minutes. The maximum amount of activities that the patient engaged in was `r max(accel_df$activity)`. 
Overall, the patient's maximum amount of minutes for any day was `r max(accel_df$activity_cnt)` minutes. 


Now let's use our tidied data set to understand the total activity over the day for each week.
We will aggregate across minutes to create a total activity variable for each day and create a 
table showing these totals. 

From the table below, we can see that there is no inherent trend of minutes per 
day and week. However, it appears that minutes engaged in activities from Tuesday 
to Thursday are pretty similar.

```{r}
accel_df %>% 
  group_by(day, week) %>% 
  summarize(total_acitvity_cnt = sum(activity_cnt)) %>% 
  pivot_wider(
    names_from = day,
    values_from = total_acitvity_cnt
  ) %>% 
  knitr::kable(digits = 1)
```


Now, let's create a single panel plot that shows the 24 hour activity courses for each day 
and use color to indicate the day of the week. 
From this graph, we can see that the maximum amount of activities and minutes of 
activities appear to be during the weekend and Monday. Nevertheless, the majority
of activities were performed in less than 2,500 minutes. 

```{r}
accel_df %>% 
  ggplot(aes(x=activity, y =activity_cnt, color = day)) +
  geom_line(alpha = .3) 
  
```

##Problem 3 

Let's load the `ny_noaa` dataset to answer problem 3 using the code below.

```{r}
data("ny_noaa") 
```
Wtite short description describing dataset, size and structure, variables missing data

about one half of the stations report precipitation only. Both the record length and period of record vary by station and cover intervals ranging from less than a year to more than 175 years.


Now we perform some data cleaning 
ensure that observations for temperature, precipitation, and snowfall are given in reasonable units
Let's perform some data cleaning on this dataset. We first separate the variable
`date` into three separate variables, `year`, `month`, and `day`. Convert 

precipitation converted to mm (0.1mm = 0.01cm) divide by 10
snowfall converted to cm(1mm = 0.1cm) divide by 10
temperature converted to C (0.1 C = 1C) divide by 10
tmax same conversions too

```{r}
noaa_df = 
  ny_noaa %>% 
  as_tibble(ny_noaa)%>% 
  janitor::clean_names() %>% 
  separate(date, into = c("year", "month", "day")) %>% 
  mutate(
      month= month.name[as.numeric(month)],
      year = as.numeric(year),
      tmax = as.numeric(tmax),
      tmin = as.numeric(tmin)) %>% 
  mutate(
    tmax = tmax / 10,
    tmin = tmin / 10,
    prcp = prcp / 10,
    snow = snow / 10
  ) 
```

Let's find the count of most common values for snow.

```{r}
noaa_df %>% 
  count(snow, name = "n_obs")
```

The most commonly observed value for snowfall observed from this dateset is 0.0cm. 
This is probably becuase it doesn't snow a lot in NY. There are many days where 
NY doesn;t receive a ton of snow. 
 New York state weather stations from January 1, 1981 through December 31, 2010.



Now let's make a two panel plot showing the average max temperature in January and in
July in each station across years. 

```{r}
noaa_df %>% 
  group_by(month,year) %>%
  filter(month == c("01","07")) %>% 
  summarize(
   mean_tmax = mean(tmax, na.rm = TRUE)) %>% 
  ggplot(aes(x =year, y = mean_tmax, color = month)) +
  geom_boxplot(alpha = .5) +
   facet_grid(. ~month)
 
  
```
Now, we make a two-panel plot showing tmax vs tmin for the full dataset. 
```{r}


```

Let's make aother plot showing the distribtuion of snowfall values greater than 0 and less 
than 100 separately by year.

```{r}
noaa_df %>% 
  group_by(year, snow) %>% 
  filter(snow < 100) %>% 
  summarize()
```









