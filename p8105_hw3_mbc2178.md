p8105_hw3_mbc2178
================
Melvin Coleman
2022-10-11

Let’s load in the packages needed to perform data import,manipulation
and cleaning for this assignment.

``` r
library(tidyverse)
library(dplyr)
library(readr)
library(p8105.datasets)
library(ggridges)
library(patchwork)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1

Let’s load the `instacart` data from the `p8105.datasets` using the
following:

``` r
data("instacart") 

instacart = 
  instacart %>% 
  as_tibble(instacart)
```

The `instacart` dataset contains 1384617 observation and 15 variables.
The variables that exist in this dataset include identifiers, orders,
aisle and name of products. There are 39123 products found in 131209
orders from 131209 buyers.

There are 134 aisles and fresh vegetables and fruits are by far the most
ordered items. The table below provides this count.

``` r
instacart %>% 
  count(aisle) %>% 
  arrange(desc(n))
## # A tibble: 134 × 2
##    aisle                              n
##    <chr>                          <int>
##  1 fresh vegetables              150609
##  2 fresh fruits                  150473
##  3 packaged vegetables fruits     78493
##  4 yogurt                         55240
##  5 packaged cheese                41699
##  6 water seltzer sparkling water  36617
##  7 milk                           32644
##  8 chips pretzels                 31269
##  9 soy lactosefree                26240
## 10 bread                          23635
## # … with 124 more rows
```

Now, let’s make a plot that shows the number of items ordered in each
aisle, limiting this to aisles with more than 10000 items ordered
arranged in ascending order by aisles.

``` r
instacart %>% 
  count(aisle) %>% 
  filter(n > 10000) %>% 
  mutate(aisle = fct_reorder(aisle, n)) %>% 
  ggplot(aes(x = aisle, y = n)) + 
  geom_point() + 
  labs(title = "Number of items ordered in each aisle") +
  theme(axis.text.x = element_text(angle = 60, hjust = 1))
```

<img src="p8105_hw3_mbc2178_files/figure-gfm/unnamed-chunk-4-1.png" width="90%" />
The table below shows the three most popular items in each aisles
“baking ingredients”, “dog food care”, and “packaged vegetables fruits”.
We show the name and count of the most popular items per category and
rank them in the table below.

``` r
instacart %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>%
  group_by(aisle) %>% 
  count(product_name) %>% 
  mutate(rank = min_rank(desc(n))) %>% 
  filter(rank < 4) %>% 
  arrange(desc(n)) %>%
  knitr::kable()
```

| aisle                      | product_name                                  |    n | rank |
|:---------------------------|:----------------------------------------------|-----:|-----:|
| packaged vegetables fruits | Organic Baby Spinach                          | 9784 |    1 |
| packaged vegetables fruits | Organic Raspberries                           | 5546 |    2 |
| packaged vegetables fruits | Organic Blueberries                           | 4966 |    3 |
| baking ingredients         | Light Brown Sugar                             |  499 |    1 |
| baking ingredients         | Pure Baking Soda                              |  387 |    2 |
| baking ingredients         | Cane Sugar                                    |  336 |    3 |
| dog food care              | Snack Sticks Chicken & Rice Recipe Dog Treats |   30 |    1 |
| dog food care              | Organix Chicken & Brown Rice Recipe           |   28 |    2 |
| dog food care              | Small Dog Biscuits                            |   26 |    3 |

Last, we make a table showing the mean hour of the day at which Pink
Lady Apples and Coffee Ice Cream are ordered on each day of the week.
Coffee Ice Cream seem to be ordered more frequently earlier on in the
day.

``` r
instacart %>%
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>%
  group_by(product_name, order_dow) %>%
  summarize(mean_hour = mean(order_hour_of_day)) %>%
  spread(key = order_dow, value = mean_hour) %>%
  knitr::kable(digits = 2)
## `summarise()` has grouped output by 'product_name'. You can override using the
## `.groups` argument.
```

| product_name     |     0 |     1 |     2 |     3 |     4 |     5 |     6 |
|:-----------------|------:|------:|------:|------:|------:|------:|------:|
| Coffee Ice Cream | 13.77 | 14.32 | 15.38 | 15.32 | 15.22 | 12.26 | 13.83 |
| Pink Lady Apples | 13.44 | 11.36 | 11.70 | 14.25 | 11.55 | 12.78 | 11.94 |

## Problem 2

Let’s import the `accel_data` dataset to R. This dataset contains five
weeks of accelorometer data collected on a 63 year old male with BMI 25,
who was admitted to the Advanced Cardiac Care Center of Columbia
University Medical Center and diagnosed with congestive hear failure
(CHF).

We will perform some tidying and wrangle the data before proceeding to
perform any analysis. First, we will consider using
`janitor::clean_names()` to clean the names of the variables in the
dataset. Next, we will use the `pivot_longer` function to create a new
variable that contains the activity count names and another variable
that contains the activity counts for each munute of a 24-hr day
starting at midnight. We arranged dataset according to day of week and
converted day to a factor variable for

``` r
accel_df =
  read_csv(file = "data/accel_data.csv") %>% 
  janitor::clean_names() %>% 
  pivot_longer(
    activity_1:activity_1440,
    names_to = "activity",
    names_prefix= "activity_",
    values_to = "activity_cnt"
  ) %>% 
  mutate(
    day = factor(day,levels =(c("Monday","Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"))),
    activity = as.numeric(activity),
    
    day_of_week = case_when(
    day == "Monday" ~ "Weekday",
    day == "Tuesday" ~ "Weekday",
    day == "Wednesday" ~ "Weekday",
    day == "Thursday" ~ "Weekday",
    day =="Friday" ~ "Weekend",
    day == "Saturday" ~ "Weekend",
    day == "Sunday" ~ "Weekend",
  )) %>% 
  arrange(day_of_week)
## Rows: 35 Columns: 1443
## ── Column specification ────────────────────────────────────────────────────────
## Delimiter: ","
## chr    (1): day
## dbl (1442): week, day_id, activity.1, activity.2, activity.3, activity.4, ac...
## 
## ℹ Use `spec()` to retrieve the full column specification for this data.
## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.
```

The `accel_df` consists of 50400 observations and 6 variables. The
variables in this data set include day id, week, number of activity and
activity counts measured in minutes. The maximum amount of activities
that the patient engaged in was 1440. Overall, the patient’s maximum
amount of minutes for any day was 8982 minutes.

Now let’s use our tidied data set to understand the total activity over
the day for each week. We will aggregate across minutes to create a
total activity variable for each day and create a table showing these
totals.

From the table below, we can see that there is no inherent trend of
minutes per day and week. However, it appears that minutes engaged in
activities from Tuesday to Thursday are pretty similar.

``` r
accel_df %>% 
  group_by(day, week) %>% 
  summarize(total_acitvity_cnt = sum(activity_cnt)) %>% 
  pivot_wider(
    names_from = day,
    values_from = total_acitvity_cnt
  ) %>% 
  knitr::kable(digits = 1)
## `summarise()` has grouped output by 'day'. You can override using the `.groups`
## argument.
```

| week |   Monday |  Tuesday | Wednesday | Thursday |   Friday | Saturday | Sunday |
|-----:|---------:|---------:|----------:|---------:|---------:|---------:|-------:|
|    1 |  78828.1 | 307094.2 |    340115 | 355923.6 | 480542.6 |   376254 | 631105 |
|    2 | 295431.0 | 423245.0 |    440962 | 474048.0 | 568839.0 |   607175 | 422018 |
|    3 | 685910.0 | 381507.0 |    468869 | 371230.0 | 467420.0 |   382928 | 467052 |
|    4 | 409450.0 | 319568.0 |    434460 | 340291.0 | 154049.0 |     1440 | 260617 |
|    5 | 389080.0 | 367824.0 |    445366 | 549658.0 | 620860.0 |     1440 | 138421 |

Now, let’s create a single panel plot that shows the 24 hour activity
courses for each day and use color to indicate the day of the week. From
this graph, we can see that the maximum amount of activities and minutes
of activities appear to be during the weekend and Monday. Nevertheless,
the majority of activities were performed in less than 2,500 minutes.

``` r
accel_df %>% 
  ggplot(aes(x=activity, y =activity_cnt, color = day)) +
  geom_line(alpha = .3) 
```

<img src="p8105_hw3_mbc2178_files/figure-gfm/unnamed-chunk-9-1.png" width="90%" />

\##Problem 3

Let’s load the `ny_noaa` dataset to answer problem 3 using the code
below.

``` r
data("ny_noaa") 
```

The data set `ny_noaa` contains 2595176 and 7. There are lots of missing
values in this data set. Reasons for this could be due to the fact that
about one half of the stations from which the data was collected
reported precipitation only. In addition, the duration of record keeping
varies by stations and those intervals range from less than a year to
more than 175 years.

We perform some data cleaning and tidying for this dataset using the
code below. We ensured that precipitation (`prcp`) and snowfall (`snow`)
were converted to millimeters as well as maximum temperature (`tmax`)
and minimum temperature (`tmin`) to degrees Celsius. Furthermore, we
separated the variable`date` into three separate variables, `year`,
`month`, and `day`. We have data from 1981-01-01 to 2010-12-31.

``` r
noaa_df = 
  ny_noaa %>% 
  as_tibble(ny_noaa)%>% 
  janitor::clean_names() %>% 
  separate(date, into = c("year", "month", "day")) %>% 
  mutate(
      month= month.name[as.numeric(month)],
      year = as.numeric(year),
      tmax = as.numeric(tmax),
      tmin = as.numeric(tmin)) %>% 
  mutate(
    tmax = tmax / 10,
    tmin = tmin / 10,
    prcp = prcp / 10,
    snow = snow / 10
  ) 
```

Let’s find the count of most common values for snowfall (`snow`). The
table below shows that 0 mm is the most common observation. This is
probably because from January 1, 1981 through December 31, 2010,the
weather in New York was not prone to heavy snow fall during the winter
and during the other seasonal months the expected snowfall count is 0mm.
Global warming and climate change could result in changes in these
observations; however,that data was not explored in this problem.

``` r
noaa_df %>% 
  count(snow, name = "n_obs") %>% 
  arrange(desc(n_obs))
## # A tibble: 282 × 2
##     snow   n_obs
##    <dbl>   <int>
##  1   0   2008508
##  2  NA    381221
##  3   2.5   31022
##  4   1.3   23095
##  5   5.1   18274
##  6   7.6   10173
##  7   0.8    9962
##  8   0.5    9748
##  9   3.8    9197
## 10   0.3    8790
## # … with 272 more rows
```

Now let’s make a two panel plot showing the average max temperature in
January and in July in each station across years. From the graph below,
the average temperature in January as expected over the years ranges
from below 0 to 10 degrees Celsius, with a few exceptions of outliers.
For example, in 1982, the temperature dropped below -10 degrees Celsius.

In addition, in July as expected the average temperature was around 25
and 30 degrees Celsius. A few outliers were observed, for example, in
1987, the average temperature was around 15 degrees Celsius.

``` r
noaa_df %>% 
  filter(month == c("January","July")) %>% 
  group_by(month,year, id) %>% 
  summarize(
   mean_tmax = mean(tmax, na.rm = TRUE)) %>% 
  mutate(year = as.numeric(year)) %>% 
  
  ggplot(aes(x= year, y = mean_tmax, group = id)) +
  geom_point(size = .5, color = "blue") +
  facet_grid(~ month) 
## `summarise()` has grouped output by 'month', 'year'. You can override using the
## `.groups` argument.
## Warning: Removed 5972 rows containing missing values (geom_point).
```

<img src="p8105_hw3_mbc2178_files/figure-gfm/unnamed-chunk-13-1.png" width="90%" />

Now, we make a two-panel plot showing tmax vs tmin for the full data
set.

``` r
tmx_min_p =
noaa_df %>%
  ggplot(aes(x = tmin, y = tmax)) +
  geom_hex() +
  theme(legend.position = "none")
```

Let’s make another plot showing the distribution of snowfall values
greater than 0 and less than 100 separately by year.The distribution of
snowfall across the years on average appear to be the relatively stable

``` r
snow_p= 
noaa_df %>%
  filter(snow < 0:100) %>% 
  ggplot(aes(x = snow, group =year, fill = year )) +
  geom_density(alpha = 0.5) + 
  facet_wrap(~year)
## Warning in snow < 0:100: longer object length is not a multiple of shorter
## object length
```

``` r
tmx_min_p + snow_p
## Warning: Removed 1136276 rows containing non-finite values (stat_binhex).
```

<img src="p8105_hw3_mbc2178_files/figure-gfm/unnamed-chunk-16-1.png" width="90%" />
